
# pre-trained language modelì„ í™œìš©í•œ í•œêµ­ì–´ ë°ì´í„° ì¦ê°•
BART, BERT, T5 ê¸°ë°˜ ì–¸ì–´ ëª¨ë¸ì„ í™œìš©í•œ í•œêµ­ì–´ ë°ì´í„° ì¦ê°•ë²• <br>
Environments
```
python: 3.8.18
OS: Windows 11
```
## 1. BART-based Korean Text data Augmentation
[Korean-CommonGen](https://github.com/J-Seo/Korean-CommonGen) ê³¼ 
[CHEF in the Language Kitchen: A Generative Data Augmentation Leveraging Korean Morpheme Ingredients](https://aclanthology.org/2023.emnlp-main.367/) 
ë…¼ë¬¸ì„ ì°¸ê³ í•œ mbart-50-large ê¸°ë°˜ í•œêµ­ì–´ í…ìŠ¤íŠ¸ ë°ì´í„° ì¦ê°•ë²•.<br>

![í˜•íƒœì†Œ](https://github.com/ghko99/korean-augmentation/assets/115913818/f9591f3e-dfdf-4b7f-ac0f-a541ba7f6314)

í•œêµ­ì–´ëŠ” ì‹¤ì§ˆí˜•íƒœì†Œì™€ í˜•ì‹í˜•íƒœì†Œë¡œ ë‚˜ë‰˜ë©°, í˜•ì‹í˜•íƒœì†Œë¥¼ ë°”ê¿”ê°€ë©° ì˜ë¯¸ëŠ” ë™ì¼í•œ ë‹¤ì–‘í•œ ë¬¸ì¥ì„ ìƒì„±í•  ìˆ˜ ìˆìŒ.<br>
í•´ë‹¹ íŠ¹ì„±ì„ ì´ìš©í•´ BARTê¸°ë°˜ í•œêµ­ì–´ ë°ì´í„° ì¦ê°•ì„ êµ¬í˜„í•  ìˆ˜ìˆìŒ.

### 1.1 Data Augmentation algorithm
![chef](https://github.com/ghko99/korean-augmentation/assets/115913818/b4638235-996a-4403-943c-7b0e54e56be5)

mecab í˜•íƒœì†Œ ë¶„ì„ê¸° í™œìš©í•´ ì–´íœ˜ í˜•íƒœì†Œ setì„ ìƒì„±í›„, mbart-large ëª¨ë¸ë¡œ í•™ìŠµ.
### 1.2 How to train mbart model
```
python mbart_train.py
```
í•™ìŠµ datasetì€ [KLUE](https://huggingface.co/datasets/klue)ì˜ Semantic Text Similarity(STS) ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•¨.
### 1.3 Arguments
```python
class ModelArguments:
    model_name_or_path = "facebook/mbart-large-50"
    setproc_model = "mbart_train"
    use_fast_tokenizer = True
    use_auth_token = False
    cache_dir = None
    model_revision = "main"

class DataTrainingArguments:
    train_file = './data/train.json'
    validation_file = './data/valid.json'
    test_file = './data/valid.json'
    preprocessing_num_workers=None
    overwrite_cache = False
    max_source_length = 1024  
    max_target_length = 128  
    pad_to_max_length = False
    num_beams = 10
    num_return_sequences = 10  
    ignore_pad_token_for_loss = True
```

### 1.4 Trained Model
ğŸ¤— [ghko99/mbart50-large-for-aug](https://huggingface.co/ghko99/mbart50-large-for-aug)
```python
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained("ghko99/mbart50-large-for-aug")
model = AutoModelForSeq2SeqLM.from_pretrained("ghko99/mbart50-large-for-aug")
```
![ìŠ¤í¬ë¦°ìƒ· 2024-03-02 122036](https://github.com/ghko99/Korean-Text-Data-Augmentation/assets/115913818/aa2376ef-7439-403b-890d-c43b5b5b00fd)

### 1.5 Usage 
```python
from mbart_aug import mbart_augment

sent_list = ["ë¬´ì—‡ë³´ë‹¤ë„ í˜¸ìŠ¤íŠ¸ë¶„ë“¤ì´ ë„ˆë¬´ ì¹œì ˆí•˜ì…¨ìŠµë‹ˆë‹¤.",
            "ì£¼ìš” ê´€ê´‘ì§€ ëª¨ë‘ ê±¸ì–´ì„œ ì´ë™ê°€ëŠ¥í•©ë‹ˆë‹¤.",
            "ê°€ì¡± ëª¨ì„ ì¼ì •ì€ ë°”ê¾¸ì§€ ë§ë„ë¡ í•˜ì‹­ì‹œì˜¤."]

aug_sents = mbart_augment(sent_list)
print(aug_sents)
```

### 1.6 Example Output
```
>>> [['ë¬´ì—‡ë³´ë‹¤ í˜¸ìŠ¤íŠ¸ê°€ ë§¤ìš° ì¹œì ˆí•˜ê³  ì„¸ì‹¬í•©ë‹ˆë‹¤.'],['ì£¼ìš” ê´€ê´‘ì§€ë¥¼ ê±¸ì–´ì„œ ì´ë™ ê°€ëŠ¥í•©ë‹ˆë‹¤.'],['ê°€ì¡±ëª¨ì„ ì¼ì •ì„ ë°”ê¾¸ì§€ ë§ë„ë¡ í•˜ì„¸ìš”.']]
```


## 2. BERT-based Korean Text data Augmentation

### 2.1 BERTì˜ Masked Language Modelì„ í™œìš©í•œ ë°ì´í„° ì¦ê°• (Random Masking Replacement).
BERTì˜ MLM(Masked Language Model)ì€ ë¬¸ë§¥ì„ ê³ ë ¤í•œ ìœ ì˜ì–´ êµì²´ì˜ êµ¬í˜„ì„ ê°€ëŠ¥í•˜ê²Œí•¨.
![bert](https://github.com/ghko99/korean-augmentation/assets/115913818/909c993b-c5f2-4327-a439-fbf119c4452f)

#### 2.1.1 í™œìš© ì‚¬ë¡€
[Data Augmentation using Pre-trained Transformer Models](https://arxiv.org/abs/2003.02245)ì—ì„œëŠ” ì˜ì–´ ë°ì´í„°ì—ì„œ BERTì˜ Masked Language Modelì„ í™œìš©í•´ Synonym Replacement(Random Masking Replacement)ë¥¼ êµ¬í˜„í•¨.<br>
[í•œêµ­ì–´ ìƒí˜¸ì°¸ì¡°í•´ê²°ì„ ìœ„í•œ End-to-end ìƒí˜¸ì°¸ì¡°í•´ê²° ëª¨ë¸ê³¼ ë°ì´í„° ì¦ê°• ë°©ë²•](https://www.dbpia.co.kr/journal/detail?nodeId=T15773139)ì—ì„œëŠ” í•œêµ­ì–´ ë°ì´í„°ì—ì„œ BERTì˜ Random Masking Replacementë¥¼ í™œìš©.<br>
ì½”ë“œì˜ ìƒë‹¹ë¶€ë¶„ì€ [MLM-data-augmentation](https://github.com/seoyeon9646/MLM-data-augmentation)ì„ ì°¸ê³ í•¨.

### 2.2 BERTì˜ Masked Language Modelì„ í™œìš©í•œ ë°ì´í„° ì¦ê°• (Random Masking Insertion).
[K-TACC(Korean Text Augmentation Considering Context)](https://github.com/kyle-bong/K-TACC)ì—ì„œëŠ” BERTì˜ Masked Language Modelì„ í™œìš©í•´ Random Insertion(Random Masking Insertion)ì„ êµ¬í˜„í•¨. <br>
ê¸°ì¡´ Replacementì˜ ê²½ìš° ë°˜ì˜ì–´ë¡œ êµì²´í•˜ëŠ” ê²½ìš°ê°€ ìˆê¸° ë•Œë¬¸ì— ì˜ë¯¸ë³´ì¡´ì´ í˜ë“¤ê³ , í•œêµ­ì–´ í† í°í™” ê¸°ë²•ì˜ íŠ¹ì„±ìœ¼ë¡œ ì¸í•´ ì–´ìƒ‰í•œ ë¬¸ì¥ì´ ì—¬ì „íˆ ì¡´ì¬í•¨. <br>
í•˜ì§€ë§Œ í•œêµ­ì–´ëŠ” ì˜ì–´ì— ë¹„í•´ ì–´ìˆœì´ ìœ ì—°í•˜ê¸° ë•Œë¬¸ì— ë¬¸ë§¥ì„ ì•Œ ìˆ˜ ìˆë‹¤ë©´ ë‹¨ì–´ ì‚¬ì´ì— ë“¤ì–´ê°ˆë§Œí•œ í† í°ì„ ì°¾ê¸°ê°€ ì‰¬ì›€. ë”°ë¼ì„œ Masked Language Modelì„ í™œìš©í•  ê²½ìš° íŠ¹ì • í† í°ì„ [MASK]í† í°ìœ¼ë¡œ ëŒ€ì²´í•˜ëŠ” ê²ƒ ë³´ë‹¤ ê¸°ì¡´ í† í°ë“¤ì€ ìœ ì§€í•œì±„ ì‚½ì…í•˜ëŠ”ê²ƒì´ ë”ìš± ì˜ë¯¸ë³´ì¡´ì´ ê°€ëŠ¥í•˜ë©° ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ì´ ë‚˜ì˜¤ê¸° ì‰¬ì›€.<br>
### 2.3 Data Augmentation algorithm (Random Masking Replacement)
#### 2.3.1 Pre-training
![mlm_pretrained](https://github.com/ghko99/korean-augmentation/assets/115913818/cb40497d-6efe-43f7-a305-a7b1f7eb0f07)
#### 2.3.2 How to pretrain model
```
python mlm_train.py
```
#### 2.3.3 Pre-trained Model
ğŸ¤— [ghko99/KR-ELECTRA-generator-for-aug](https://huggingface.co/ghko99/KR-ELECTRA-generator-for-aug)
```python
from transformers import AutoTokenizer, AutoModelForMaskedLM

tokenizer = AutoTokenizer.from_pretrained("ghko99/KR-ELECTRA-generator-for-aug")
model = AutoModelForMaskedLM.from_pretrained("ghko99/KR-ELECTRA-generator-for-aug")
```
#### 2.3.4 Random Masking Replacement

![mlm](https://github.com/ghko99/korean-augmentation/assets/115913818/a14b309d-7b24-4c19-b562-01c9ceeeebe6)
#### 2.3.5 Usage
```python
from mlm_augmentation import mlm_replace_augment
from transformers import AutoModelForMaskedLM,AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("ghko99/KR-ELECTRA-generator-for-aug")
model = AutoModelForMaskedLM.from_pretrained("ghko99/KR-ELECTRA-generator-for-aug").cuda()

sent = "ë¬´ì—‡ë³´ë‹¤ë„ í˜¸ìŠ¤íŠ¸ë¶„ë“¤ì´ ë„ˆë¬´ ì¹œì ˆí•˜ì…¨ìŠµë‹ˆë‹¤."
aug_sent = mlm_replace_augment(model=model,tokenizer=tokenizer,sent=sent,mlm_prob=0.15)
print(aug_sent)
```
#### 2.3.6 Example Output
```
>>> ë¬´ì—‡ë³´ë‹¤ë„ í˜¸ìŠ¤íŠ¸ë¶„ì´ ë§¤ìš° ì¹œì ˆí•˜ì…¨ìŠµë‹ˆë‹¤.
```
### 2.4 Data Augmentation algorithm (Random Masking Insertion)
#### 2.4.1 Random Masking Insertion

![rmi](https://github.com/ghko99/korean-augmentation/assets/115913818/a132f064-a49e-44b5-9d86-88eb676873b8)
ì•ì„œ pretrainedëœ ëª¨ë¸ì„ ì‚¬ìš©í•´ Random Masking Insertionì„ ìˆ˜í–‰
#### 2.4.2 Usage
```python
from mlm_insert_augmentation import mlm_insert_augment
from transformers import AutoModelForMaskedLM,AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained('ghko99/KR-ELECTRA-generator-for-aug')
model = AutoModelForMaskedLM.from_pretrained('ghko99/KR-ELECTRA-generator-for-aug').cuda()

sent = "ë¬´ì—‡ë³´ë‹¤ë„ í˜¸ìŠ¤íŠ¸ë¶„ë“¤ì´ ë„ˆë¬´ ì¹œì ˆí•˜ì…¨ìŠµë‹ˆë‹¤."
aug_sent = mlm_insert_augment(model=model,tokenizer=tokenizer,sent=sent,ratio=0.15)
print(aug_sent)
```
#### 2.4.3 Example Output
```
>>> ë¬´ì—‡ë³´ë‹¤ë„ í˜¸ìŠ¤íŠ¸ë¶„ë“¤ì´ ì •ë§ ë„ˆë¬´ ì¹œì ˆí•˜ì…¨ìŠµë‹ˆë‹¤.
```
## 3. T5-based Korean Text data Augmentation
T5ì˜ Paraphrase taskë¡œ ì…ë ¥ë¬¸ì¥ê³¼ ì˜ë¯¸ëŠ” ê°™ì§€ë§Œ ë‹¤ë¥¸ ë¬¸ì¥ì„ ìƒì„±

![T5](https://github.com/ghko99/korean-augmentation/assets/115913818/7cf18e7e-7044-4540-a5e2-da9f8a37ba5d) <br>
Huggingfaceì˜ [psyche/KoT5-paraphrase-generation](https://huggingface.co/psyche/KoT5-paraphrase-generation/discussions)ì„ í™œìš©í•¨
### 3.1 Usage
```python
from t5_augmentation import t5_augment
from transformers import pipeline

pipe = pipeline("text2text-generation", model="psyche/KoT5-paraphrase-generation", device=0)
sent_list = ["ë¬´ì—‡ë³´ë‹¤ë„ í˜¸ìŠ¤íŠ¸ë¶„ë“¤ì´ ë„ˆë¬´ ì¹œì ˆí•˜ì…¨ìŠµë‹ˆë‹¤.",
            "ì£¼ìš” ê´€ê´‘ì§€ ëª¨ë‘ ê±¸ì–´ì„œ ì´ë™ê°€ëŠ¥í•©ë‹ˆë‹¤.",
            "ê°€ì¡± ëª¨ì„ ì¼ì •ì€ ë°”ê¾¸ì§€ ë§ë„ë¡ í•˜ì‹­ì‹œì˜¤."]
aug_sents = t5_augment(sent_list=sent_list,pipe=pipe,batch_size=64)
print(aug_sents)
```
### 3.2 Example Output
```
>>> ['ë¬´ì—‡ë³´ë‹¤ë„ í˜¸ìŠ¤íŠ¸ë¶„ë“¤ì´ ë„ˆë¬´ ì¹œì ˆí•˜ì…¨ìŠµë‹ˆë‹¤.','ì£¼ìš” ê´€ê´‘ì§€ë¥¼ ê±¸ì–´ì„œ ëª¨ë‘ ê°ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.','ê°€ì¡± ëª¨ì„ ì¼ì •ì€ ë³€ê²½í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.']
```

## 4. Augmentation sample files
[KLUE](https://huggingface.co/datasets/klue)ì˜ Semantic Text Similarity(STS) ë°ì´í„°ì…‹ì„ 4ê°€ì§€ ì¦ê°•ë°©ì‹ìœ¼ë¡œ 1:1 ë¹„ìœ¨(2ë°°)ë¡œ ì¦ê°•í•¨. <br><br>
Random Masking Replacement ì¦ê°• sample íŒŒì¼: 
[mapping_electra.json](https://github.com/ghko99/Korean-Text-Data-Augmentation/blob/master/data/mapping_electra.json)<br>
Random Masking Insertion ì¦ê°• sample íŒŒì¼: 
[mapping_insert.json](https://github.com/ghko99/Korean-Text-Data-Augmentation/blob/master/data/mapping_insert.json)<br>
mbart50-large ì¦ê°• sample íŒŒì¼: 
[mapping_mbart.json](https://github.com/ghko99/Korean-Text-Data-Augmentation/blob/master/data/mapping_mbart.json)<br>
t5 ì¦ê°• sample íŒŒì¼: 
[mapping_t5.json](https://github.com/ghko99/Korean-Text-Data-Augmentation/blob/master/data/mapping_t5.json)<br>
## 5. Augmentation ì„±ëŠ¥ ì¸¡ì •
[K-TACC](https://github.com/kyle-bong/K-TACC)ì˜ STS ì„±ëŠ¥ ì¸¡ì • ì½”ë“œë¥¼ í™œìš©í•´ ë°ì´í„° ì¦ê°•ì˜ ì„±ëŠ¥ ì¸¡ì •ì„ ì§„í–‰í•¨. <br>
baseline modelì€ [monologg/koelectra-base-v3-discriminator](https://huggingface.co/monologg/koelectra-base-v3-discriminator)ì„ í™œìš©í–ˆìŒ.
### 5.1 training arguments:
```
data:
    shuffle: True
    augmentation: True
    max_length : 128

train:
  seed: 42
  batch_size: 32
  max_epoch: 4
  learning_rate: 5e-5
  logging_step: 100
  drop_out: 0.1
  warmup_ratio: 0.1
  weight_decay: 0.01

runner:
  options:
    num_workers: 8
```
### 5.2 KLUE STS ì¦ê°•ì…‹ ìƒì„±
```
python augmentation.py
```
### 5.3 STS ì„±ëŠ¥ ì¸¡ì •
```
sh train.sh
```
### 5.4 STS ì„±ëŠ¥ ì¸¡ì • ê²°ê³¼
|           Augmentation           | Pearson's correlation |
|:--------------------------------:|:---------------------:|
|             Baseline             |   0.9241138696670532  |
| (BERT) Random Masking Replacemnt |   0.8990367650985718  |
|  (BERT) Random Masking Insertion |   0.9256289601325989  |
|       (BART) mbart50-large       |   0.9271315336227417  |
|  (T5) KoT5-paraphrase-generation |   0.915034294128418   |

BARTì™€ BERTì˜ Random Masking Insertion ì¦ê°•ë²•ì´ STSì—ì„œ ê°€ì¥ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ëŠ”ê²ƒìœ¼ë¡œ ë³´ì„.
## 6. Reference
* [K-TACC](https://github.com/kyle-bong/K-TACC)
* [KLUE](https://huggingface.co/datasets/klue)
* [psyche/KoT5-paraphrase-generation](https://huggingface.co/psyche/KoT5-paraphrase-generation/discussions)
* [Data Augmentation using Pre-trained Transformer Models](https://arxiv.org/abs/2003.02245)
* [í•œêµ­ì–´ ìƒí˜¸ì°¸ì¡°í•´ê²°ì„ ìœ„í•œ End-to-end ìƒí˜¸ì°¸ì¡°í•´ê²° ëª¨ë¸ê³¼ ë°ì´í„° ì¦ê°• ë°©ë²•](https://www.dbpia.co.kr/journal/detail?nodeId=T15773139)
* [MLM-data-augmentation](https://github.com/seoyeon9646/MLM-data-augmentation)
* [monologg/koelectra-base-v3-discriminator](https://huggingface.co/monologg/koelectra-base-v3-discriminator)
